---
title: "Actividad 2 - Regresión Lineal Múltiple"
author: "Alan Robledo, Diego Romano, Gonzalo Farias"
date: "11/11/2023"
output:
  html_document:
    toc: yes
    toc_depth: 2
  pdf_document:
    toc: yes
    toc_depth: '2'
lang: es-AR    
---
[Fuente: https://worldhappiness.report/data/](https://worldhappiness.report/data/)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción

![Fuente de Datos](IMG/source.png)

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(dplyr) 
library(xtable)      
library(ggplot2)  
library(corrplot)
library(readxl)
library(leaps) 
library(car)
library(lmtest)
library(broom)
library(fmsb)
```

# Preparación de los Datos

```{r}
# Cargar de datos
datos <- read_excel("felicidad_mundial.xls")
datos <- subset(datos, year == 2014) # Establecemos un solo año para hacer nuestra regresión.
datos <- subset(datos, select = -c(1,2))

# Eliminamos las columnas "País","año" y renombramos las columnas para mayor claridad
colnames(datos)<-c("SatisfaccionVida", "LogPBI", "ApoyoSocial", "EsperanzaVidaSaludableNacer", "LibertadTomarDecisionesVida", "Generosidad", "PercepcionCorrupcion", "AfectoPositivo", "AfectoNegativo", "ConfianzaGobiernoNacional")
View(datos)

# BÚSQUEDA DE DATOS NULOS
# Visualizamos un resumen de datos nulos en el conjunto de datos.
# View(summarise_all(datos, funs(sum(is.na(.)))))
datos <- na.omit(datos) # Eliminamos las filas con valores nulos en caso de haber

# CASTEO DE DATOS
# Convertimos todas las columnas a tipo numérico para asegurarnos de que sean interpretables.
attach(datos)
datos <- datos %>% mutate_all(as.numeric)
```

# Análisis Exploratorio de los Datos

```{r, message=FALSE, warning=FALSE}
summary(datos)  # Estadísticas descriptivas

## Vemos correlaciones y graficamos:
datos_cor <- cor(datos)  # Calculamos y almacenamos las correlaciones
corrplot(datos_cor, method = "number", tl.col = "black", tl.cex = 0.8)
plot(datos)
```

# Generación de Modelos
En la generación de los modelos, usamos métodos automáticos para seleccionar las variables para la regresión. Intentamos hacerlo en 3 direcciones diferentes: fordward (hacia delante), backward (hacia atrás) y both (ambas)

Mostramos el sumario con los datos descriptivos de cada uno de ellos para comparar y tomar una decisión.

```{r, message=FALSE, warning=FALSE}
## Metodos automaticos para seleccion de variables, probamos cual es mejor:
##  "backward" / "forward" / "both"
mod_full<-lm(SatisfaccionVida~.,data=datos)
summary(mod_full)

summary(step(mod_full, direction = "forward",trace=F) )
summary(step(mod_full, direction = "backward",trace=F))
summary(step(mod_full, direction = "both",trace=F))
```

# Elección de un Modelo
El modelo elegido fue el generado en dirección "backward" (hacia atrás) porque si bien nos dió un coeficiente de determinación un poco menor: 0,7904 contra  0,7944 del modelo generado en dirección "fordward", lo elegimos porque el estadístico f nos da mejor: 63,03 contra 49,38.  El p valor es muy pequeño: 2.2e-16 en todos los casos.

Las variables "Generosidad" y "AfectoNegativo" quedan fuera. Por lo tanto, de las 9 variables predictoras originales nos quedamos solo con 7.

El histograma de los residuos manifiesta una forma aproximada de campana, lo cual indica la normalidad de los mismos.

```{r, message=FALSE, warning=FALSE}
# Elegimos el modelo
modelo <- step(mod_full, direction = "backward",trace=T) # Optamos por el modelo con backward que nos dió mejores resultados

residuos = residuals(modelo)
summary(residuos)

hist(residuos) # Vemos si se da la campana de Gauss
```

# Verificación de Supuestos de RLM
Ahora verificamos los supuestos del modelo de regresión lineal múltiple. Para mayor claridad, disponemos los gráficos de una matriz de 2x2.

Demostramos la linealidad de los residuos

```{r, message=FALSE, warning=FALSE}
par(mfrow=c(2,2))
plot(modelo) 
par(mfrow=c(1,1))

# 1. Linealidad: Dispersion sin patron alguno de residuos, viendo plots del modelo y residuos
plot(residuos)

# 2. homocedasticidad: verificar la homocedasticidad graficando los residuos estandarizados
# Grafico de Scale-Location
# Crear un boxplot personalizado debe tener mediana 0 (de residuos)
boxplot(residuos,
        main = "Boxplot de Residuos",
        xlab = "Variable X",
        ylab = "Variable Y",
        col = "lightblue",
        border = "blue",
        horizontal = TRUE)

# 3. Homogeneidad de la varianza: observar el gráfico de residuos vs. valores predichos. (Residuals vs Fitted)
# Si los puntos se distribuyen aleatoriamente alrededor de la línea de referencia, se cumple la suposición de homogeneidad de varianza.

# 4. Independencia de los residuos: usar test de Durbin Watson
# Errores independientes (no correlacionados, es equivalente si hay normalidad)
# Test Durbin Watson
# DW debe ser cercano a 2 y el p-value cercano a 1 para demostrarlo
# un pvalue muy pequenio indica que no hay independencia (están correlacionados)
# La hipotesis nula es que no hay autocorrelacion.
dwtest(modelo)  # Si el pvalor es chico, entonces hay dependencias entre residuos

# 5. Normalidad de los residuos: usar un gráfico QQplot 
# Test de normalidad de Shapiro-Wilk (muestras chicas)
# Cuanto más cercano esté W a 1, más probable es que los datos se ajusten a una distribución normal. 
# Si el p-valor es mayor que el nivel de significancia (ejemplo 0.05), se asume que los datos siguen una distribución normal.
x.test <- shapiro.test(residuos)
print(x.test)

# 6. Multicolinealidad e influyentes:
# MULTICOLINEALIDAD
# VIF: Si es mayor a 10, entonces hay correlacion entre variables
# VIF = 1: NO HAY MULTICOLINEALIDAD
# VIF > 1: MULTICOLINEALIDAD ACEPTABLE
# VIF > 5: MULTICOLINEALIDAD ALTA
vif(modelo) 

#INFLUYENTES:
cooks=cooks.distance(modelo)
plot(cooks.distance(modelo))

# Resumen del modelo
summary(modelo)
# Si el valor de F es grande y el valor p es chico indica que el modelo de regresión es significativo 
# y explica de manera efectiva la variabilidad en los datos.
# Se busca un alto R cuadrado ajustado y significatividad en las variables.
```

# Conclusión

```{r, message=FALSE, warning=FALSE}
## PREDICCION de nuevos datos
nuevo <- data.frame(
  LogPBI = 7.65,
  ApoyoSocial = 0.52,
  EsperanzaVidaSaludableNacer = 53.2,
  LibertadTomarDecisionesVida = 0.50,
  Generosidad = 0.10,
  PercepcionCorrupcion = 0.87,
  AfectoPositivo = 0.49,
  AfectoNegativo = 0.37,
  ConfianzaGobiernoNacional = 0.40
)
#valores a predecir
predict(modelo,nuevo)
# Pusimos datos casi identicos a la primera fila de los originales y nos dió como resultado
# un valor similar, el valor original es de 3.13 y la predicción de 3.29
```
En conclusión, la regresión lineal múltiple realizada revela que la satisfacción de vida puede ser
explicada en gran medida por factores como el Producto Interno Bruto per cápita, el apoyo social,
la esperanza de vida saludable al nacer, la libertad para tomar decisiones de vida, la percepción de
corrupción, el afecto positivo y la confianza en el gobierno nacional. El modelo resultante muestra un
buen ajuste a los datos, con un R cuadrado ajustado de 0.7779, lo que indica una capacidad predictiva
sólida. Este estudio respalda la utilidad de la regresión
lineal múltiple como una herramienta efectiva para entender y predecir la satisfacción de vida en función
de múltiples variables.


